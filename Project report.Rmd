---
title: "Energy Usage - kWh"
author: "Aamodini Gupta"
date: "12/3/2018"
output: html_document
---

```{r, results='hide'}
library(ggplot2)
library(randomForest)
library(tree)
library(glmnet)
library(MASS)
```

Load the dataset

```{r}
dat.old <- read.csv("~/Documents/Hogwarts/Graduate UIUC/First Year/STAT 432/energy-usage-2010.csv")
```

Note that there are a lot of NA values in this dataset. So first we take those out (there are NA values in the categorical variables). This makes sense because randomly assigning values to the categorical variables could give false information and mess up the analysis. 


```{r}
dat <- dat.old[complete.cases(dat.old),]
(1 - nrow(dat)/nrow(dat.old))*100 
```

Since the dataset is very large, taking out about `31.5%` of the data still leaves us with a substantial amount of the data. 

Note that a lot of the columns in the dataset give information about the total, standard deviation, quartiles and such for the kWh usage per household over the 12 months. Want to take this out cause those can't be a part of the predictor variables.

```{r}
#the labels are bit weird so I've just redefined them here.
dat$BUILDING.TYPE <- droplevels(dat$BUILDING.TYPE)
dat$BUILDING.TYPE <- as.factor(dat$BUILDING.TYPE)

dat$BUILDING_SUBTYPE <- droplevels(dat$BUILDING_SUBTYPE)
dat$BUILDING_SUBTYPE <- as.factor(dat$BUILDING_SUBTYPE)

dat$ELECTRICITY.ACCOUNTS <- droplevels(dat$ELECTRICITY.ACCOUNTS)
dat$ELECTRICITY.ACCOUNTS <- as.factor(dat$ELECTRICITY.ACCOUNTS)

#just for kWh
kwh.dat <- dat[, which(colnames(dat) %in% c("COMMUNITY.AREA.NAME",
                                            "CENSUS.BOCK",
                                            "BUILDING.TYPE",
                                            "BUILDING_SUBTYPE",
                                            "ELECTRICITY.ACCOUNTS",
                                            "ZERO.KWH.ACCOUNTS",
                                            "KWH.SQFT.MEAN.2010",
                                            "TOTAL.POPULATION",
                                            "TOTAL.UNITS",
                                            "AVERAGE.STORIES",
                                            "AVERAGE.BUILDING.AGE",
                                            "AVERAGE.HOUSESIZE",
                                            "OCCUPIED.UNITS",
                                            "RENTER.OCCUPIED.HOUSING.UNITS",
                                            "OCCUPIED.HOUSING.UNITS",
                                            "KWH.MEAN.2010"))]
```

Now the data is given in a way that the total usage per month is given row-wise instead of a column of data that is classified by month. So for each observation, instead of having kWh usage row-wise make it column-wise and create a new column `months` that is a categorical variable that gives information about the months.

```{r}
#just for plotting purpose make categories characters
lab <- c('Jan', 'Feb', 'Mar', 'Apr','May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')
labs <- sapply(1:12, function(x) rep(lab[x], nrow(kwh.dat)))

months <- c('JANUARY',
            'FEBRUARY',
            'MARCH',
            'APRIL',
            'MAY',
            'JUNE',
            'JULY',
            'AUGUST',
            'SEPTEMBER',
            'OCTOBER',
            'NOVEMBER',
            'DECEMBER')

colnames <- sapply(1:length(months), function(x) paste0('KWH', '.', months[x], '.2010'))
months.dat <- dat[, which(colnames(dat) %in% colnames)] #months dataset for kWh

kwh.fin <- data.frame(kwh.dat, "KWH.USE" = months.dat[,1], "months" = labs[,1])

for (mon in 2:ncol(months.dat)){
  ds <- data.frame(kwh.dat, "KWH.USE" = months.dat[,mon], "months" = labs[,mon])
  kwh.fin <- rbind(kwh.fin, ds)
}
```

I can just add the column for seasons as well cause why not. (I don't think there is need to do this - can omit this chunk. Just here for reference if we need to add in the seasons.)

```{r}
kwh.fin <- data.frame(kwh.fin, 'season' = 1)

for (labels in lab) {
  if (labels == 'Mar'){
    kwh.fin[which(kwh.fin$months == "Mar"), ]$season = 2
  }
  if (labels == 'Apr'){
    kwh.fin[which(kwh.fin$months == "Apr"), ]$season = 2
  }
  if (labels == 'May'){
    kwh.fin[which(kwh.fin$months == "May"), ]$season = 2
  }
  if (labels == 'Jun'){
    kwh.fin[which(kwh.fin$months == "Jun"), ]$season = 3
  }
  if (labels == 'Jul'){
    kwh.fin[which(kwh.fin$months == "Jul"), ]$season = 3
  }
  if (labels == 'Aug'){
    kwh.fin[which(kwh.fin$months == "Aug"), ]$season = 3
  }
  if (labels == 'Sep'){
    kwh.fin[which(kwh.fin$months == "Sep"), ]$season = 4
  }
  if (labels == 'Oct'){
    kwh.fin[which(kwh.fin$months == "Oct"), ]$season = 4
  }
  if (labels == 'Nov'){
    kwh.fin[which(kwh.fin$months == "Nov"), ]$season = 4
  }
}

kwh.fin$season <- as.factor(kwh.fin$season)
```


View the datasets

```{r}
ggplot(kwh.fin, aes(x=BUILDING.TYPE)) +
  geom_bar()
```

Look at how there are virtually nothing that is categorised in the `Industrial` building type. We can take out the variables that are categorized as `Industrial`

```{r}
#takes out the category from both datasets that we have
kwh.dat <- kwh.dat[kwh.dat$BUILDING.TYPE != "Industrial",]
kwh.dat$BUILDING.TYPE <- factor(kwh.dat$BUILDING.TYPE)
kwh.fin <- kwh.fin[kwh.fin$BUILDING.TYPE != "Industrial",]
kwh.fin$BUILDING.TYPE <- factor(kwh.fin$BUILDING.TYPE)
```

Look at the building type graph now:

```{r}
ggplot(kwh.fin, aes(x=BUILDING.TYPE)) +
  geom_bar()
```

What about the building subtype graph:

```{r}
ggplot(kwh.fin, aes(x=BUILDING_SUBTYPE)) +
  geom_bar()
```

See again how municipal is virtually nothing compared to the rest? Take this out too

```{r}
#takes out the category from both datasets that we have
kwh.dat <- kwh.dat[kwh.dat$BUILDING_SUBTYPE != "Municipal",]
kwh.dat$BUILDING_SUBTYPE <- factor(kwh.dat$BUILDING_SUBTYPE)
kwh.fin <- kwh.fin[kwh.fin$BUILDING_SUBTYPE != "Municipal",]
kwh.fin$BUILDING_SUBTYPE <- factor(kwh.fin$BUILDING_SUBTYPE)
```

Now:

```{r}
ggplot(kwh.fin, aes(x=BUILDING_SUBTYPE)) +
  geom_bar()
```

This shows that there are too many levels for electricity accounts. There are some spikes here and there. Can we find a better way to classify this variable?

```{r}
ggplot(kwh.dat, aes(x=ELECTRICITY.ACCOUNTS)) +
  geom_bar()
```

```{r}
# values <- levels(kwh.dat$ELECTRICITY.ACCOUNTS)
# values <- values[-length(values)]
# values <- as.numeric(values)
# values <- sort(values) # so there are 272 values other than Less than 4.

# just rename the levels -> this also works!

cat.1 <- as.character(values[1:3])
cat.2 <- as.character(values[4:6])
cat.3 <- as.character(values[7:10])
cat.4 <- as.character(values[11:18])
cat.5 <- as.character(values[19:34])
cat.6 <- as.character(values[35:272])

# cat.1 <- as.character(values[1:34])
# cat.2 <- as.character(values[35:68])
# cat.3 <- as.character(values[69:102])
# cat.4 <- as.character(values[103:136])
# cat.5 <- as.character(values[137:170])
# cat.6 <- as.character(values[171:204])
# cat.7 <- as.character(values[205:238])
# cat.8 <- as.character(values[239:272])
```

```{r}
for (lev in 1:length(levels(kwh.dat$ELECTRICITY.ACCOUNTS))){
  if (levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.1 == T){
    levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] = "4 to 6"
  } else if (levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.2 == T){
    levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] = "7 to 9"
  } else if (levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.3 == T){
    levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] = "10 to 13"
  } else if (levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.4 == T){
    levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] = "14 to 21"
  } else if (levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.5 == T){
    levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] = "22 to 37"
  } else if (levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.6 == T){
    levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] = "38 and Greater"
  } 
  # else if (levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.7 == T){
  #   levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] = "257 and Greater"
  # } 
  # else if (levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.8 == T){
  #   levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] = "257 and Greater"
  # }
}
```


```{r}
# for (lev in 1:length(levels(kwh.dat$ELECTRICITY.ACCOUNTS))){
#   if (levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.1 == T){
#     levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] = "4 to 37"
#   }
#   if (levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.2 == T){
#     levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] = "38 to 71"
#   }
#   if (levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.3 == T){
#     levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] = "72 to 105"
#   }
#   if (levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.4 == T){
#     levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] = "106 to 143"
#   }
#   if (levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.5 == T){
#     levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] = "145 to 191"
#   }
#   if (levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.6 == T){
#     levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] = "194 to 254"
#   }
#   if (levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.7 == T){
#     levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] = "257 to 393"
#   }
#   if (levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.8 == T){
#     levels(kwh.dat$ELECTRICITY.ACCOUNTS)[lev] = "412 and Greater"
#   }
# }
```



```{r}
ggplot(kwh.fin, aes(x=COMMUNITY.AREA.NAME)) +
  geom_bar()
```

Okay these are still too many. Let's see if we can make this smaller.

```{r}
Central <- c("Near North Side", "Loop", "Near South Side")
North.Side <- c("North Center", "Lakeview", "Lincoln Park", "Avondale", "Logan Square")
Far.North.Side <- c("Rogers Park", "West Ridge", "Uptown", "Lincoln Square", "Edison Park", "Norwood Park", "Jefferson Park", "Forest Glen","North Park", "Albany Park","O'Hare","Edgewater")
Northwest.Side <- c("Portage Park", "Irving Park", "Dunning", "Montclare", "Belmont Cragin","Hermosa")
West.Side <- c("Humboldt Park", "West Town", "Austin", "West Garfield Park", "East Garfield Park", "Near West Side", "North Lawndale", "South Lawndale", "Lower West Side")
South.Side <- c("Armour Square", "Douglas", "Oakland", "Fuller Park", "Grand Boulevard", "Kenwood", "Washington Park", "Hyde Park", "Woodlawn", "South Shore", "Bridgeport", "Greater Grand Crossing")
Southwest.Side <- c("Garfield Ridge","Archer Heights", "Brighton Park", "McKinley Park", "New City", "West Elsdon", "Gage Park", "Clearing", "West Lawn", "Chicago Lawn", "West Englewood", "Englewood")
Far.Southeast.Side <- c("Chatham", "Avalon Park", "South Chicago", "Burnside", "Calumet Heights", "Roseland", "Pullman", "South Deering", "East Side", "West Pullman", "Riverdale", "Hegewisch")
Far.Southwest.Side <- c("Ashburn", "Auburn Gresham", "Beverly", "Washington Heights", "Mount Greenwood", "Morgan Park")
```

```{r}
kwh.dat <- data.frame(kwh.dat, "Community.Area" = 0)
kwh.dat$Community.Area[1] = "Far.North.Side"
for (row in 1:nrow(kwh.dat)){
  if (kwh.dat$COMMUNITY.AREA.NAME[row] %in% Central == T){
    kwh.dat$Community.Area[row] = "Central"
  }
  if (kwh.dat$COMMUNITY.AREA.NAME[row] %in% North.Side == T){
    kwh.dat$Community.Area[row] = "North.Side"
  }
  if (kwh.dat$COMMUNITY.AREA.NAME[row] %in% Far.North.Side == T){
    kwh.dat$Community.Area[row] = "Far.North.Side"
  }
  if (kwh.dat$COMMUNITY.AREA.NAME[row] %in% Northwest.Side == T){
    kwh.dat$Community.Area[row] = "Northwest.Side"
  }
  if (kwh.dat$COMMUNITY.AREA.NAME[row] %in% West.Side == T){
    kwh.dat$Community.Area[row] = "West.Side"
  }
  if (kwh.dat$COMMUNITY.AREA.NAME[row] %in% South.Side == T){
    kwh.dat$Community.Area[row] = "South.Side"
  }
  if (kwh.dat$COMMUNITY.AREA.NAME[row] %in% Southwest.Side == T){
    kwh.dat$Community.Area[row] = "Southwest.Side"
  }
  if (kwh.dat$COMMUNITY.AREA.NAME[row] %in% Far.Southeast.Side == T){
    kwh.dat$Community.Area[row] = "Far.Southeast.Side"
  }
  if (kwh.dat$COMMUNITY.AREA.NAME[row] %in% Far.Southwest.Side == T){
    kwh.dat$Community.Area[row] = "Far.Southwest.Side"
  }
}
kwh.dat$Community.Area <- as.factor(kwh.dat$Community.Area)
```

```{r}
ggplot(kwh.dat, aes(x=Community.Area)) +
  geom_bar()
```

Now to look at the data with kWh usage:

```{r}
ggplot(kwh.fin, aes(x=KWH.USE)) + geom_histogram()
```

Need to use the logarithmic scale on `KWH.USE`.

```{r}
kwh.fin$KWH.USE <- log(1+kwh.fin$KWH.USE)

ggplot(kwh.fin, aes(x=KWH.USE)) + 
  geom_histogram(binwidth=1, color="black", fill="orange") 
```

Should be similar case for KWH.Mean

```{r}
ggplot(kwh.fin, aes(x=KWH.MEAN.2010)) + 
  geom_histogram() 
```

```{r}
kwh.fin$KWH.MEAN.2010 <- log(1+kwh.fin$KWH.MEAN.2010)
kwh.dat$KWH.MEAN.2010 <- log(1+kwh.dat$KWH.MEAN.2010)

ggplot(kwh.fin, aes(x=KWH.MEAN.2010)) + 
  geom_histogram(binwidth=1, color="black", fill="orange") 
```

Now look at some line graphs to look at the distribuitions:

```{r}
ggplot(data=kwh.fin, aes(x=months, y=KWH.USE, group=1)) +
  xlab("Months") + ylab("Average kWh Consumption") + ggtitle("Average kWh Consumption per Month") +
  geom_line(stat='summary', fun.y='mean') +
  geom_point(stat='summary', fun.y='mean')

ggplot(data=kwh.fin, aes(x=months, y=KWH.USE, group=BUILDING_SUBTYPE, color = BUILDING_SUBTYPE)) +
  xlab("Month") + ylab("kWh Consumption") + ggtitle("kWh Consumption by Building Subtype per Month") +
  geom_line(stat='summary', fun.y='mean') +
  geom_point(stat='summary', fun.y='mean')

ggplot(data=kwh.fin, aes(x=AVERAGE.HOUSESIZE, y=KWH.MEAN.2010, group=1)) +
  xlab("Average House Size") + ylab("kWh Mean") +
  geom_line(stat='summary', fun.y='mean') +
  geom_point(stat='summary', fun.y='mean')
```

Try to make some more pairwise plots and heatmaps

```{r}

```

# PCA and Lasso

The goal of doing PCA and then Lasso is to see which variables contribute in predicting the kWh usage.

## PCA

```{r}
#levels(kwh.fin$months) <- c('1', '2', '3', '4','5', '6', '7', '8', '9', '10', '11', '12')
nums <- unlist(lapply(kwh.dat, is.numeric))
dat.numeric <- kwh.dat[ , nums]
dat.numeric <- as.matrix(dat.numeric)
numeric.pca <- princomp(dat.numeric)
plot(numeric.pca, type = "l", pch = 19, main = "Engergy Use PCA Variance")
hist(log(1+numeric.pca$scores[,1]))
```

This shows that most of the variance is explained by the first component. This doesn't work because this either means that the dataset it highly correlated - in which case having 10 columns is the same as having 1 column - or that the values of the dataset are so scattered and different from one another that we can't really see whats going on in the other components because the values themselves are just way too high for component 1. One of the things that gives away the fact that it might be the latter problem is that the values of the variance has the power of 8. So if I standardize each column of the dataset (this would simply be a tansformation of the dataset) and run the PCA on that - this way I'll have all my variables scaled at mean 0. 

PCA based on corelation matrix. So work on normalizing the dataset.

```{r}
for (col in 1:ncol(dat.numeric)){
  dat.numeric[,col] <- (dat.numeric[,col] - mean(dat.numeric[,col]))/sd(dat.numeric[,col])
}
numeric.pca.norm <- princomp(dat.numeric)
plot(numeric.pca.norm, type = "l", pch = 19, main = "Engergy Use PCA Variance")
hist(log(1+numeric.pca.norm$scores[,1]))
```

The normalized data gives a lot more information about the corelations! We can now look at the loadings to decide how and where we must look! Choosing components 1-8 seems like the proper course of action. 

```{r}
prcomp.pca <- prcomp(dat.numeric)
ggplot(data = data.frame(prcomp.pca$x), aes(x=PC1, y=PC2)) +
      geom_point(color=c("chartreuse4", "darkorange")[kwh.dat$BUILDING.TYPE], size = 1)

ggplot(data = data.frame(prcomp.pca$x), aes(x=PC1, y=PC2)) +
      geom_point(color=c("chartreuse4", "darkorange", "purple", "red")[kwh.dat$BUILDING_SUBTYPE], size = .5)
```

Since we have a large number of vairables. Try to get some form of interpretations from these representations.

```{r}
features = row.names(prcomp.pca$rotation)
ggplot(data = data.frame(prcomp.pca$rotation), aes(x=PC1, y=PC2, label=features,color=features)) +
    geom_point(size = 3) + geom_text(size=3)
```

To see how many components we want to keep, look at the loadings so we know how many components give us about 80% of the variation in the dataset.

```{r}
numeric.pca.norm$loadings
```

Cumulative variance till component 8 seems sufficient. 

Outside of plotting, you could look at the max values of the loadings.

```{r}
max.val <- sapply(1:11, function (x) which.max(abs(prcomp.pca$rotation[,x])))
n <- names(max.val) #there are in order of most variance explained.
# take the first 8 components
n[1:8]
```

PC1 <- So there are a lot of corelation between `OCCUPIED.HOUSING.UNITS`, `RENTER.OCCUPIED.HOUSING.UNITS`, `OCCUPIED.UNITS`, `TOTAL.UNITS` and `TOTAL.POPULATION` - So this essentially tells us that PC1 is describing the housing population. 

PC2 <- Highest are `KWH.SQFT.MEAN.2010` and `AVERAGE.STORIES` - describes housing size.

PC3 <- Highest is `AVERAGE.BUILDING.AGE` - inefficient buildings.

So putting these in the model will give us information about the energy use.

```{r}
pca.linearfit <- data.frame("PC1" = prcomp.pca$x[,1], "PC2" = prcomp.pca$x[,2], "PC3" = prcomp.pca$x[,3], kwh.dat[, (nums == F)], "KWH.MEAN.2010" = kwh.dat$KWH.MEAN.2010)
pca.linearfit <- pca.linearfit[,-which(colnames(pca.linearfit) == "COMMUNITY.AREA.NAME")]
pca.linearfit <- pca.linearfit[,-which(colnames(pca.linearfit) == "ELECTRICITY.ACCOUNTS")]

pca.fit <- lm(KWH.MEAN.2010 ~., dat = pca.linearfit)
summary(pca.fit)
```

Rsquared with electric accounts and residential is a lot higher ~ .715
Rsquared with only electric ~ .6955
Rsquared with only res ~ .6762
Rsquared without both ~ .6496 

But these are too many categorical variables (overfitting). Maybe some sort of bias-variance tradeoff answer to help with selecting the variables. 

Which leads us to question - can we reclassify the number of electric accounts? There are `378` different levels in electric accounts. Or maybe even consider taking out "Less than 4"

Get the pairwise plots.
Take 4 PC's

```{r}
pairs(prcomp.pca$x[, 1:3], col=c("chartreuse4", "darkorange", "deepskyblue", "purple")[kwh.dat$BUILDING_SUBTYPE], cex = .3, pch = 19)
```

This gives us more information so we can check which numerical variables we need to look out for.

# Regressions

Now we need to pick between Lasso and Ridge Regression. One helps with model selection, the other simply penalizes - the higher the variables are correlated, the more the penalization.

##### This is for reference

The dataset we have so far:

- dat.old <- this is the unprocessed dataset
- dat <- this is the complete dataset (without NAN values)
- dat.numeric <- this is the dataset with only the numerical variables from kwh.dat
- kwh.dat <- this is the dataset with the all the kWh information WITHOUT month (KWH.MEAN.2010 is pred)
- kwh.fin <- this is the dataset with the kWh information WITH month (KWH.USE is pred)
- pca.imp.dat <- this is the dataset with both the categorical and numerical variables (numerical variables that was shown to explain most of the variance in the dataset) plus the y value.


## Lasso

Define the dataset: 

```{r}
pca.imp.dat <- kwh.dat[, which(colnames(kwh.dat) %in% c("AVERAGE.HOUSESIZE", "KWH.SQFT.MEAN.2010", "AVERAGE.BUILDING.AGE", "ZERO.KWH.ACCOUNTS", "AVERAGE.STORIES", "RENTER.OCCUPIED.HOUSING.UNITS"))]
pca.imp.dat <- data.frame(pca.imp.dat, kwh.dat[, (nums == F)], "KWH.MEAN.2010" = kwh.dat$KWH.MEAN.2010)
```

```{r}
set.seed(3)
lasso_all <- cv.glmnet(data.matrix(pca.imp.dat[,1:10]), pca.imp.dat$KWH.MEAN.2010, nfolds = 10)
coef(lasso_all, s = "lambda.1se")
par(mfrow = c(1, 2))
plot(lasso_all)
plot(lasso_all$glmnet.fit, "lambda")
#lambda is being tuned.
min(lasso_all$lambda)
```

According to Lasso Regression, the parameters that predict KWH.MEAN.2010 are:
"AVERAGE.HOUSESIZE", "KWH.SQFT.MEAN.2010", "AVERAGE.BUILDING.AGE", "ZERO.KWH.ACCOUNTS", "AVERAGE.STORIES", "RENTER.OCCUPIED.HOUSING.UNITS", "BUILDING.TYPE", "BUILDING_SUBTYPE"

**The coeffients are awfully small. Maybe we can compare this Lasso to other AIC/BIC model selection methods to see how each perform on just a simple lmfit?.**

**What can we use to then pick between the two model selections**

**Do we get the same result if we use all the data (kwh.dat)?**

```{r}
set.seed(3)
lasso_kwh <- cv.glmnet(data.matrix(kwh.dat[,-which(colnames(kwh.dat) == "KWH.MEAN.2010")]), kwh.dat$KWH.MEAN.2010, nfolds = 10)
coef(lasso_kwh, s = "lambda.1se")
par(mfrow = c(1, 2))
plot(lasso_kwh)
plot(lasso_kwh$glmnet.fit, "lambda")
#lambda is being tuned.
min(lasso_kwh$lambda)
```

**Why are the two model selections different? The lambda is smaller here?.**

Can I try to use kwh.fin?

```{r}
months.kwh.lasso <- kwh.fin[,-which(colnames(kwh.fin) == "KWH.MEAN.2010")]
set.seed(3)
lasso_kwh <- cv.glmnet(data.matrix(months.kwh.lasso[,-which(colnames(months.kwh.lasso) == "KWH.USE")]), months.kwh.lasso$KWH.USE, nfolds = 10)
coef(lasso_kwh, s = "lambda.1se")
par(mfrow = c(1, 2))
plot(lasso_kwh)
plot(lasso_kwh$glmnet.fit, "lambda")
#lambda is being tuned.
min(lasso_kwh$lambda)
```


## Ridge

**Does ridge ALSO need to be standardized? Does this work with categorical variables? Why am I getting an error with lm.ridge?**

```{r}
fit = lm.ridge(KWH.MEAN.2010 ~., data.frame(dat.numeric), lambda=seq(10,100,by=0.1))

par(mfrow=c(1,2))
matplot(coef(fit)[, -1], type = "l", xlab = "Lambda", ylab = "Coefficients")
text(rep(50, 8), coef(fit)[1,-1])
title("Prostate Cancer Data: Ridge Coefficients")

# use GCV to select the best lambda
plot(fit$lambda[1:500], fit$GCV[1:500], type = "l", col = "darkorange", 
     ylab = "GCV", xlab = "Lambda", lwd = 3)
title("Prostate Cancer Data: GCV")
round(coef(fit)[which.min(fit$GCV), ], 4)

```

**Can try this if nothing else works**
Can split by month - but won't get month effect
Repeated data^^^

tuning lambda
add interaction terms in ridge if model doesn't do well.

# Random Forest

```{r}
rf.bt.fit <- randomForest(dat.numeric, kwh.dat$BUILDING_SUBTYPE, ntree = 1000, mtry = 1, nodesize = 20, sampsize = 500)
rf.bt.fit
```

The classification error from `Multi < 7` and `Single Family` have the lowest oob errors. 

Tuning the rf to give the best errors

```{r}
tuneRF(dat.numeric, kwh.dat$BUILDING_SUBTYPE, ntreeTry=500)
```

Using mtry = 3 gives the smallest out-of-bag error so using that, rerun rf 

```{r}
rf.fit.tune <- randomForest(dat.numeric, kwh.dat$BUILDING_SUBTYPE, ntree = 500, mtry = 3, nodesize = 1)
rf.fit.tune
```

This classifaction is SO much better - use ntree = 500, choosing the default `sampsize` also makes a difference. 

Went from 3 to 6 when i changed ntree for tuneRF. 


```{r}
tuneRF(dat.numeric, kwh.dat$BUILDING_SUBTYPE, ntreeTry=500)
```

Shows that `mtry = 6` could be better. 

```{r}
rf.fit.tune <- randomForest(dat.numeric, kwh.dat$BUILDING_SUBTYPE, ntree = 500, mtry = 6, nodesize = 1)
rf.fit.tune
```

Okay so `mtry = 6` isn't improving it more. Keeping `mtry = 3` gives the best classifcation error. Well I can't recreate the best one we've gotten. So sticking with `ntree = 1000`, `mtry = 3` and `nodesize = 1` gives the best OOB error of `~9.7%`

```{r}
rf.fit.tune.fin <- randomForest(dat.numeric, kwh.dat$BUILDING_SUBTYPE, ntree = 1000, mtry = 6, nodesize = 1)
rf.fit.tune.fin
plot(rf.fit.tune.fin)
mean(predict(rf.fit.tune.fin) != kwh.dat$BUILDING_SUBTYPE)
```

Look at a different way of tuning.

```{r}
nodes <- c(1, 20, 40, 60) #picking some node values to test
mtrys <- c(1, 3, 6) #keeping the center value as the default 
ntrees <- c(200, 500, 1000)
best <- c(10,10,10, 10) #initating the best(error, mtry, nodesize)

for (ntree in ntrees){
  print(ntree)
  for (nodesize in nodes){
    print(nodesize)
    for (mtry in mtrys){
      print(mtry)
      rf.fit <- randomForest(dat.numeric, kwh.dat$BUILDING_SUBTYPE, 
                             ntree = ntree, mtry = mtry, nodesize = nodesize)
      OOB.err <- mean(predict(rf.fit) != kwh.dat$BUILDING_SUBTYPE)
      #retain the information of only those predictors that have the lowest prediction error
      ifelse(OOB.err < best[1], best <- c(OOB.err, mtry, nodesize, ntree), best <- best)
      print(best)
    }
  }
}
```

This code gives the best error at 9.7% with `mtry = 6`, `nodesize = 1`, and `nodesize = 1000`








