---
title: "Analysis of Factors that Affect Gas and Electricity Usage in Chicago"
author: "Aamodini Gupta (gupta88) and Sahiti Kolli (kolli4)"
date: "12/13/2018"
output: pdf_document
header-includes:
  - \usepackage{indentfirst}
  - \usepackage{booktabs}
  - \setlength{\parindent}{3em}
  - \setlength{\parskip}{1em}
---

```{r echo=FALSE, message=FALSE}
library(ggplot2)
library(randomForest)
library(tree)
library(glmnet)
library(MASS)
library(leaps)
library(captioner)
library(knitr)
library(xtable)
library(gridExtra)
library(splines)
library(Metrics)
```

# Introduction and Literature Review

Buildings represent 40% of energy consumption and 38% of CO2 emissions in the United States (Amasyali & El-Gohary, 2018). Understanding energy consumption can aid in implementing energy efficient planning and create infrastructure and environments that reduce inefficient energy usage. Energy saving practices can aid in the effort to reduce air pollution, especially in large urban areas. Additionally, understanding energy consumption can minimize economic losses since forecasting has become a tool for optimizing energy resources. Predicting and understanding energy consumption is dependent on a variety of diverse factors including properties of the building, weather, and behavior of the occupants. Thus, it is challenging to accurately determine the specific factors of variations in energy usage. 

This analysis aims to understand the factors that predict energy expenditure by comparing the results obtained from linear regression and basis spline models. The analysis also aims to understand if building types can be accurately classified using various factors including energy expenditure through the use of random forests. Through this analysis, the aim is to have a better understanding of what specific factors of buildings and occupants affect energy expenditure as well as how various factors differ across building types. 

The data comes from open source data from the city of Chicago found [here](https://data.cityofchicago.org/Environment-Sustainable-Development/Energy-Usage-2010/8yq3-m6wp). It displays units of energy consumption for households, businesses, and industries in the City of Chicago during 2010 measured by electricity (kWh) and gas (Therm) expenditure. Each observation is a building, and for each observation, total and average energy expenditure for 2010 is provided along with the average expenditure for each month in the year. The data is aggregated from ComEd and Peoples Natural Gas by Accenture. Electrical and gas usage data comprises 88% of Chicago's buildings in 2010. The electricity data comprises 68% of overall electrical usage in the city while gas data comprises 81% of all gas consumption in Chicago for 2010. The analysis will compare both electricity and gas usage. The data also contains variables such as Census block population, building characteristics, and occupancy for each observation. The data has 67.1K observations and 73 variables which are both categorical and numeric in nature with a size of 10 MB. 

Although there are no publicly available analyses on this specific data, there have been numerous studies to understand energy consumption using analytical and predictive methods. Previous research has successfully predicted future expenditure based on previous years’ consumption through artificial neural networks and modified Newton’s method. However, they do not assess factors affecting these expenditures (Ozoh et al., 2014). Research that does look into the factors affecting energy consumption use artificial neural networks to find that price and temperature are significant when predicting expenditure (Ozoh et al., 2016). Analysis comparing the performance between random forests and neural networks finds that the latter performed better (Tso and Yao, 2007). Improved predictive performance from using a random forest model compared to a neural network or other machine learning techniques has guided this analysis to include this type of method. 

Analysis has also been done to predict both short term and long term energy expenditure. Short term expenditure measures hours and days and is beneficial in understanding generator capacity and short term maintenance. Long term expenditure measures yearly or longer data and is beneficial in developing more permanent generation strategies (Rahman et al., 2018). A recent analysis of existing models in predicting energy finds that very few models focus on long term analysis and only 19% of reviewed studies focus on residential housing (Amasyali & El-Gohary, 2018). Since the implication of this analysis should help guide and understand more long term energy efficient practices, it is important to conduct analysis on data that covers an entire year of energy expenditure and includes information on residential housing. 

There has been considerable research into this topic and different machine learning algorithms have been used to analyze energy related data. However, through this analysis, more potential factors such as building type, age of the building, and size of the unit are included. These factors are essential in affecting the efficiency and expenditure of energy usage and have been lacking in previous analyses. Doing so creates a more comprehensive understanding of energy usage and consolidates the different analyses that have previously been done. Additionally, by analyzing both kWh and Therms, there is an assessment if certain factors are more important for one energy type over the other, thus providing more insight into understanding expenditure. 

\newpage

# Data Exploration

Exploratoration of data is done to better understand the data and its variables prior to conducting analysis. The first step is to remove the observations in the data with any missing values. Since the data has an immense number of observations, doing so reduces it by 31.5% from 67,051 to 45,854 observations. This is still a large number of observations to conduct the analysis on. 

```{r echo=FALSE, message=FALSE, results='hide'}
dat.old <- read.csv("~/Documents/Hogwarts/Graduate UIUC/First Year/STAT 432/energy-usage-2010.csv")
dat <- dat.old[complete.cases(dat.old),]
dat.1 <- dat
(1 - nrow(dat)/nrow(dat.old))*100 
```

Every observation in the data is a building with 68 numeric and 5 categorical features. The categorical variables are Building Type, Building Subtype, Electricity Accounts, Gas Accounts, and Community Area. 

The categories and number of observations in each category for Building Type and Building Subtype are shown in the table below. Building Subtypes are sub-categories of Building Types. For Building Type, there are only 2 observations for Industrial buildings. These observations are removed to prevent any model from attempting to classify a category with too few observations and to instead focus on categories with a larger sample size. The Building Subtypes show a similar result for Municipal buildings (28 observations) and are also removed. The dominating category for Building Subtype is Single Family which accounts for 51.1% of the observations. 

```{r echo=FALSE}
#the labels are bit weird so I've just redefined them here.
dat$BUILDING.TYPE <- droplevels(dat$BUILDING.TYPE)
dat$BUILDING.TYPE <- as.factor(dat$BUILDING.TYPE)

dat$BUILDING_SUBTYPE <- droplevels(dat$BUILDING_SUBTYPE)
dat$BUILDING_SUBTYPE <- as.factor(dat$BUILDING_SUBTYPE)

dat$ELECTRICITY.ACCOUNTS <- droplevels(dat$ELECTRICITY.ACCOUNTS)
dat$ELECTRICITY.ACCOUNTS <- as.factor(dat$ELECTRICITY.ACCOUNTS)

dat$GAS.ACCOUNTS <- droplevels(dat$GAS.ACCOUNTS)
dat$GAS.ACCOUNTS <- as.factor(dat$GAS.ACCOUNTS)
```

```{r echo=FALSE, fig.width=3, fig.height=2}
# For the whole dataset

# BUILDING.TYPE ---------------------------------------------------
dat <- dat[dat$BUILDING.TYPE != "Industrial",] # remove 'Industrial'
dat$BUILDING.TYPE <- factor(dat$BUILDING.TYPE)

dat$BUILDING.TYPE <- droplevels(dat$BUILDING.TYPE) # redefine the levels 
dat$BUILDING.TYPE <- as.factor(dat$BUILDING.TYPE)

# BUILDING SUBTYPE without Industry 
dat$BUILDING_SUBTYPE <- droplevels(dat$BUILDING_SUBTYPE)
dat$BUILDING_SUBTYPE <- as.factor(dat$BUILDING_SUBTYPE)

# BUILDING_SUBTYPE ---------------------------------------------------
dat <- dat[dat$BUILDING_SUBTYPE != "Municipal",] # remove 'Municipal'
dat$BUILDING_SUBTYPE <- factor(dat$BUILDING_SUBTYPE)

dat$BUILDING_SUBTYPE <- droplevels(dat$BUILDING_SUBTYPE) # redefine the levels
dat$BUILDING_SUBTYPE <- as.factor(dat$BUILDING_SUBTYPE)
```

```{r, results='asis', echo=FALSE}
t1 <- kable(t(table(dat$BUILDING.TYPE)), format = "latex")
t2 <- kable(t(table(dat$BUILDING_SUBTYPE)), format = "latex")
cat(c("\\begin{table}[h] \\centering ", 
      t1,
    "\\hspace{1cm} \\centering ",
      t2,
    "\\end{table}")) 
```

Electricity Accounts have 273 categories and Gas Accounts have 150 categories with an uneven distribution in each category. Each variable is consolidated into six categories displayed in the figure below.

```{r echo=FALSE}
# Recategorize the ELECTRICITY.ACCOUNTS ---------------------------------------------------

values <- levels(dat$ELECTRICITY.ACCOUNTS)
values <- values[-length(values)]
values <- as.numeric(values)
values <- sort(values) 

cat.1 <- as.character(values[1:3])
cat.2 <- as.character(values[4:6])
cat.3 <- as.character(values[7:10])
cat.4 <- as.character(values[11:18])
cat.5 <- as.character(values[19:34])
cat.6 <- as.character(values[35:272])

for (i in 1:20){
  for (lev in 1:length(levels(dat$ELECTRICITY.ACCOUNTS))){
    if (levels(dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.1 == T){
      levels(dat$ELECTRICITY.ACCOUNTS)[lev] = "4 to 6"
    } else if (levels(dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.2 == T){
      levels(dat$ELECTRICITY.ACCOUNTS)[lev] = "7 to 9"
    } else if (levels(dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.3 == T){
      levels(dat$ELECTRICITY.ACCOUNTS)[lev] = "10 to 13"
    } else if (levels(dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.4 == T){
      levels(dat$ELECTRICITY.ACCOUNTS)[lev] = "14 to 21"
    } else if (levels(dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.5 == T){
      levels(dat$ELECTRICITY.ACCOUNTS)[lev] = "22 to 37"
    } else if (levels(dat$ELECTRICITY.ACCOUNTS)[lev] %in% cat.6 == T){
      levels(dat$ELECTRICITY.ACCOUNTS)[lev] = "38 and Greater"
    } 
  } 
}

# Recategorize the GAS.ACCOUNTS ---------------------------------------------------

values <- levels(dat$GAS.ACCOUNTS)
values <- values[-length(values)]
values <- as.numeric(values)
values <- sort(values) 

cat.1 <- as.character(values[1:3])
cat.2 <- as.character(values[4:6])
cat.3 <- as.character(values[7:10])
cat.4 <- as.character(values[11:18])
cat.5 <- as.character(values[19:34])
cat.6 <- as.character(values[35:149])

for (i in 1:20){
  for (lev in 1:length(levels(dat$GAS.ACCOUNTS))){
    if (levels(dat$GAS.ACCOUNTS)[lev] %in% cat.1 == T){
      levels(dat$GAS.ACCOUNTS)[lev] = "4 to 6"
    } else if (levels(dat$GAS.ACCOUNTS)[lev] %in% cat.2 == T){
      levels(dat$GAS.ACCOUNTS)[lev] = "7 to 9"
    } else if (levels(dat$GAS.ACCOUNTS)[lev] %in% cat.3 == T){
      levels(dat$GAS.ACCOUNTS)[lev] = "10 to 13"
    } else if (levels(dat$GAS.ACCOUNTS)[lev] %in% cat.4 == T){
      levels(dat$GAS.ACCOUNTS)[lev] = "14 to 21"
    } else if (levels(dat$GAS.ACCOUNTS)[lev] %in% cat.5 == T){
      levels(dat$GAS.ACCOUNTS)[lev] = "22 to 37"
    } else if (levels(dat$GAS.ACCOUNTS)[lev] %in% cat.6 == T){
      levels(dat$GAS.ACCOUNTS)[lev] = "38 and Greater"
    } 
  } 
}
```

```{r, echo=FALSE, fig.width=3, fig.height=3}

par(mfrow=c(1,2))
ggplot(dat, aes(x=ELECTRICITY.ACCOUNTS)) +
  xlab("Electricity Account Categories") + ggtitle("Distribution of Categories") +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10)) +
  geom_bar(fill = "cadetblue") + coord_flip()
ggplot(dat, aes(x=GAS.ACCOUNTS)) +
  xlab("Gas Account Categories") + ggtitle("Distribution of Categories") +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10)) +
  geom_bar(fill = "steelblue3") + coord_flip()

```

Community Area also has a large number of categories. The original categories have the names of  77 neighborhoods of Chicago which are now consolidated into 9 geographic regions based on this [information](https://en.wikipedia.org/wiki/Community_areas_in_Chicago).  

```{r echo=FALSE}
# Recategorize the COMMUNITY.AREA.NAMES ---------------------------------------------------

Central <- c("Near North Side", "Loop", "Near South Side")
North.Side <- c("North Center", "Lakeview", "Lincoln Park", "Avondale", "Logan Square")
Far.North.Side <- c("Rogers Park", "West Ridge", "Uptown", "Lincoln Square", "Edison Park", 
                    "Norwood Park", "Jefferson Park", "Forest Glen","North Park", "Albany Park",
                    "O'Hare","Edgewater")
Northwest.Side <- c("Portage Park", "Irving Park", "Dunning", "Montclare", "Belmont Cragin","Hermosa")
West.Side <- c("Humboldt Park", "West Town", "Austin", "West Garfield Park", "East Garfield Park",
               "Near West Side", "North Lawndale", "South Lawndale", "Lower West Side")
South.Side <- c("Armour Square", "Douglas", "Oakland", "Fuller Park", "Grand Boulevard", "Kenwood", 
                "Washington Park", "Hyde Park", "Woodlawn", "South Shore", "Bridgeport", "Greater Grand Crossing")
Southwest.Side <- c("Garfield Ridge","Archer Heights", "Brighton Park", "McKinley Park", "New City", 
                    "West Elsdon", "Gage Park", "Clearing", "West Lawn", "Chicago Lawn", "West Englewood", "Englewood")
Far.Southeast.Side <- c("Chatham", "Avalon Park", "South Chicago", "Burnside", "Calumet Heights", 
                        "Roseland", "Pullman", "South Deering", "East Side", "West Pullman", "Riverdale", "Hegewisch")
Far.Southwest.Side <- c("Ashburn", "Auburn Gresham", "Beverly", "Washington Heights", "Mount Greenwood", "Morgan Park")

dat <- data.frame(dat, "Community.Area" = 0)
dat$Community.Area[1] = "Far.North.Side"
for (row in 1:nrow(dat)){
  if (dat$COMMUNITY.AREA.NAME[row] %in% Central == T){
    dat$Community.Area[row] = "Central"
  }
  if (dat$COMMUNITY.AREA.NAME[row] %in% North.Side == T){
    dat$Community.Area[row] = "North.Side"
  }
  if (dat$COMMUNITY.AREA.NAME[row] %in% Far.North.Side == T){
    dat$Community.Area[row] = "Far.North.Side"
  }
  if (dat$COMMUNITY.AREA.NAME[row] %in% Northwest.Side == T){
    dat$Community.Area[row] = "Northwest.Side"
  }
  if (dat$COMMUNITY.AREA.NAME[row] %in% West.Side == T){
    dat$Community.Area[row] = "West.Side"
  }
  if (dat$COMMUNITY.AREA.NAME[row] %in% South.Side == T){
    dat$Community.Area[row] = "South.Side"
  }
  if (dat$COMMUNITY.AREA.NAME[row] %in% Southwest.Side == T){
    dat$Community.Area[row] = "Southwest.Side"
  }
  if (dat$COMMUNITY.AREA.NAME[row] %in% Far.Southeast.Side == T){
    dat$Community.Area[row] = "Far.Southeast.Side"
  }
  if (dat$COMMUNITY.AREA.NAME[row] %in% Far.Southwest.Side == T){
    dat$Community.Area[row] = "Far.Southwest.Side"
  }
}
dat$Community.Area <- as.factor(dat$Community.Area)

```


There are a large number of numeric variables in the data. Several split the energy expenditures into quartiles, standard deviations, total, minimum and maximum values for each observation. Since the analysis is focused on understanding and predicting average energy usage over the year, these variables are removed. There are also 12 additional columns, 1 for each month, that give the average energy usage for every observation per month.

The average values of kWh usage have a very skewed distribution, as seen in the left plot below. This indicates that the range of values for the electric energy usage is very high. The log scale is taken to account for this. The right plot below shows the distribution of kWh expenditure after scaling, where the range is more reasonable. A similar pattern is seen in the average values of Therms, and the same scaling is applied. 

```{r echo=FALSE}
# Log Scale of Mean Values -------------------------------------------------------

dat$KWH.MEAN.2010 <- log(1+dat$KWH.MEAN.2010)
dat$THERM.MEAN.2010 <- log(1+dat$THERM.MEAN.2010)
```

```{r, echo=FALSE, fig.width=3.5, fig.height=2.5}
par(mfrow=c(1,2))
ggplot(dat.1, aes(x=KWH.MEAN.2010)) +
  xlab("Mean kWh") + ggtitle("Distribution of Mean kWh - Before") +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10)) +
  geom_histogram(bins=30, fill = "cadetblue")
ggplot(dat, aes(x=KWH.MEAN.2010)) +
  xlab("Mean kWh") + ggtitle("Distribution of Mean kWh - After") +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10)) +
  geom_histogram(bins=30, fill = "cadetblue")
```

The figures below show the average kWh and Therm consumption by month. There is a sharp increase in kWh consumption and decrease in Therm consumption during the months of June through August. Likewise, less kWh consumption and higher Therm consumption is observed in the colder months. This indicates that month and time of the year differ in terms of energy usage, so it is important to take this into account during the analysis. 

```{r echo=FALSE}
# Split the datasets

# kWh Data ----------------------------------------------------------------

# Create 2 datasets: kwh.dat and kwh.ms

kwh.dat <- dat[, which(colnames(dat) %in% c("Community.Area",
                                            "CENSUS.BOCK",
                                            "BUILDING.TYPE",
                                            "BUILDING_SUBTYPE",
                                            "ELECTRICITY.ACCOUNTS",
                                            "ZERO.KWH.ACCOUNTS",
                                            "KWH.SQFT.MEAN.2010",
                                            "TOTAL.POPULATION",
                                            "TOTAL.UNITS",
                                            "AVERAGE.STORIES",
                                            "AVERAGE.BUILDING.AGE",
                                            "AVERAGE.HOUSESIZE",
                                            "OCCUPIED.UNITS",
                                            "RENTER.OCCUPIED.HOUSING.UNITS",
                                            "OCCUPIED.HOUSING.UNITS",
                                            "KWH.MEAN.2010"))]

# Split months

lab <- c('Jan', 'Feb', 'Mar', 'Apr','May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')
labs <- sapply(1:12, function(x) rep(lab[x], nrow(kwh.dat)))

months <- c('JANUARY',
            'FEBRUARY',
            'MARCH',
            'APRIL',
            'MAY',
            'JUNE',
            'JULY',
            'AUGUST',
            'SEPTEMBER',
            'OCTOBER',
            'NOVEMBER',
            'DECEMBER')

colnames <- sapply(1:length(months), function(x) paste0('KWH', '.', months[x], '.2010'))
months.dat <- dat[, which(colnames(dat) %in% colnames)] #months dataset for kWh

kwh.ms <- data.frame(kwh.dat, "KWH.USE" = months.dat[,1], "months" = labs[,1])

for (mon in 2:ncol(months.dat)){
  ds <- data.frame(kwh.dat, "KWH.USE" = months.dat[,mon], "months" = labs[,mon])
  kwh.ms <- rbind(kwh.ms, ds)
}
# Log scale 
kwh.ms$KWH.USE <- log(1+kwh.ms$KWH.USE)
kwh.ms <- kwh.ms[,-which(colnames(kwh.ms) == "KWH.MEAN.2010")]
```


```{r echo=FALSE}
# Therms Data -------------------------------------------------------------

# Create 2 datasets: therms.dat and therms.ms

therms.dat <- dat[, which(colnames(dat) %in% c("Community.Area",
                                               "CENSUS.BOCK",
                                               "BUILDING.TYPE",
                                               "BUILDING_SUBTYPE",
                                               "GAS.ACCOUNTS",
                                               "THERMS.SQFT.MEAN.2010",
                                               "TOTAL.POPULATION",
                                               "TOTAL.UNITS",
                                               "AVERAGE.STORIES",
                                               "AVERAGE.BUILDING.AGE",
                                               "AVERAGE.HOUSESIZE",
                                               "OCCUPIED.UNITS",
                                               "RENTER.OCCUPIED.HOUSING.UNITS",
                                               "OCCUPIED.HOUSING.UNITS",
                                               "THERM.MEAN.2010"))]

# Split months
lab <- c('Jan', 'Feb', 'Mar', 'Apr','May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')
labs <- sapply(1:12, function(x) rep(lab[x], nrow(therms.dat)))

months <- c('JANUARY',
            'FEBRUARY',
            'MARCH',
            'APRIL',
            'MAY',
            'JUNE',
            'JULY',
            'AUGUST',
            'SEPTEMBER',
            'OCTOBER',
            'NOVEMBER',
            'DECEMBER')

colnames <- sapply(1:length(months), function(x) paste0('THERM', '.', months[x], '.2010'))
months.dat <- dat[, which(colnames(dat) %in% colnames)] #months dataset for kWh

therms.ms <- data.frame(therms.dat, "THERM.USE" = months.dat[,1], "months" = labs[,1])

for (mon in 2:ncol(months.dat)){
  ds <- data.frame(therms.dat, "THERM.USE" = months.dat[,mon], "months" = labs[,mon])
  therms.ms <- rbind(therms.ms, ds)
}

# Log scale 
therms.ms$THERM.USE <- log(1+therms.ms$THERM.USE)
therms.ms <- therms.ms[,-which(colnames(therms.ms) == "THERM.MEAN.2010")]
```

```{r, echo=FALSE, fig.width=7, fig.height=2.3}


p1 <- ggplot(data=kwh.ms, aes(x=months, y=KWH.USE, group=1)) +
      xlab("Months") + ylab("Avg kWh Consumption") + ggtitle("Average kWh Consumption per Month") +
      theme(plot.title = element_text(hjust = 0.5, size = 10),
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10)) +
      geom_line(stat='summary', fun.y='mean', col = "cadetblue") +
      geom_point(stat='summary', fun.y='mean', col = "cadetblue")

p2 <- ggplot(data=therms.ms, aes(x=months, y=THERM.USE, group=1)) +
      xlab("Months") + ylab("Avg Therm Consumption") + 
      ggtitle("Average Therm Consumption per Month") +
      theme(plot.title = element_text(hjust = 0.5, size = 10),
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10)) +
      geom_line(stat='summary', fun.y='mean', col = "steelblue3") +
      geom_point(stat='summary', fun.y='mean', col = "steelblue3")

grid.arrange(p1, p2, ncol=2)
```

Previous research has found temperature to be significant in predicting energy expenditure (Ozoh et al., 2016), thus providing further evidence to include the effects of month into the analysis. In addition to splitting the data into separate data sets for kWh and Therms, data sets are also created to account for months. Each of these data sets have two additional columns indicating the month and the month’s corresponding average energy for each observation. This is done by recording each observation twelve times with each energy value per month in one column and the category of the month in the next column. Creating and manipulating the original data set to obtain this format is essential for the analysis so that the effect of the months can be taken into account. 

To understand how the numeric variables contribute to the variation in the data, principal components analysis (PCA) is conducted. 

PCA aids in understanding which variables contribute to the most variation in the data. After centering the variables and doing PCA for the kWh data, the variance explained by each of the components is seen in the figure below. The first component explains the largest amount of variance in the data and the first 8 components explain 81% of the cumulative variance. 


```{r echo = FALSE}
# PCA kWh -----------------------------------------------------------------

# Get the numeric dataset
nums <- unlist(lapply(kwh.dat, is.numeric))
num.kwh <- kwh.dat[ , nums]

# standardize the datase
sd.num.kwh <- num.kwh
for (col in 1:ncol(sd.num.kwh)){
  sd.num.kwh[,col] <- (sd.num.kwh[,col] - mean(sd.num.kwh[,col]))/sd(sd.num.kwh[,col])
}

princomp.kwh <- princomp(sd.num.kwh) # Get the variance plot
prcomp.kwh <- prcomp(sd.num.kwh)

# PCA Therms  -----------------------------------------------------------------

# Get the numeric dataset
nums <- unlist(lapply(therms.dat, is.numeric))
num.therms <- therms.dat[ , nums]

# standardize the datase
sd.num.therms <- num.therms
for (col in 1:ncol(sd.num.therms)){
  sd.num.therms[,col] <- (sd.num.therms[,col] - mean(sd.num.therms[,col]))/sd(sd.num.therms[,col])
}

princomp.therms <- princomp(sd.num.therms) # Get the variance plot
prcomp.therms <- prcomp(sd.num.therms)
```

```{r echo=FALSE, fig.width=5.5, fig.height=3.5}
plot(princomp.kwh, type = "l", pch = 19, main = "Energy Use (kWh) PCA Variance", col = "cadetblue")
```

The two dimensional pairwise plots of the first three components show the greatest separation by Building Subtype, seen through the color separation. This provides evidence that there may be factors that vary across building subtypes, which is explored in the analysis. 

```{r echo=FALSE, fig.width=6, fig.height=5}
pairs(prcomp.kwh$x[, 1:3], col=c("chartreuse4", "darkorange", "deepskyblue", "purple")[kwh.dat$BUILDING_SUBTYPE], cex = .3, pch = 19, main = "Pairwise Plot for Principal Components")
```

The loadings of the first three components are shown below. Analyzing the loadings provides feature contributions of the components. The first component presents the largest absolute values of loadings in the variables describing the occupied units and total population indicating that this component is largely describing the building occupancy. The second component has the largest loading values in average square foot and average stories and is thus describing the building size. Lastly, the third component is describing building age. This is repeated for the data containing Therms and similar results are seen.  

```{r table, results='asis', echo=FALSE}
t1 <- kable(prcomp.kwh$rotation[,1:3], format = "latex")
cat(c("\\begin{table}[h] \\centering ", 
      t1,
    "\\end{table}")) 
```

The two plots below show average kWh and Therm consumption by Building Subtype. For kWh, the Commercial buildings have the highest expenditure, and households with less than 7 occupants have the lowest expenditure. For Therms, households with over 7 occupants have the highest expenditure and single family homes have the lowest expenditure. It should be noted that the energy expenditure varies across building subtype. There is evidence that energy expenditure varies across building subtype, and this provides justification for using building subtype to understand energy usage in the analysis.  

```{r, echo=FALSE, fig.width=6, fig.height=2.5}
ggplot(data=kwh.ms, aes(x=months, y=KWH.USE, group=BUILDING_SUBTYPE, color = BUILDING_SUBTYPE)) +
  xlab("Month") + ylab("kWh Consumption") + 
  ggtitle("kWh Consumption by Building Subtype per Month") +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10)) +
  geom_line(stat='summary', fun.y='mean') +
  geom_point(stat='summary', fun.y='mean')
```

```{r, echo=FALSE, fig.width=6, fig.height=2.5}
ggplot(data=therms.ms, aes(x=months, y=THERM.USE, group=BUILDING_SUBTYPE, color = BUILDING_SUBTYPE)) +
  xlab("Month") + ylab("Therm Consumption") + 
  ggtitle("Therm Consumption by Building Subtype per Month") +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10)) +
  geom_line(stat='summary', fun.y='mean') +
  geom_point(stat='summary', fun.y='mean')
```

The exploration of the data has helped identify variables that needed further processing and cleaning. It also aided in visualizing underlying trends in the data that propagated the analysis. 

\newpage

# Analysis

To determine the factors that affect both average kWh and Therm expenditure per year, two different models are analyzed and compared. 

## Linear Regression with Principal Components

### Analysis of kWh

The first model is a linear regression with the first three principal components and the four categorical variables as predictors that predict the average kWh. Principal components are used as predictors to reduce the complexity of the model, which effectively decreases the variance but also increases the bias of the model. 
$~$

```{r}
nums <- unlist(lapply(kwh.dat, is.numeric))
pca.linearfit.kwh <- data.frame("PC1" = prcomp.kwh$x[,1],"PC2" = prcomp.kwh$x[,2], 
                                "PC3" = prcomp.kwh$x[,3], kwh.dat[, (nums == F)], 
                                "KWH.MEAN.2010" = kwh.dat$KWH.MEAN.2010)
pca.fit.kwh <- lm(KWH.MEAN.2010 ~., dat = pca.linearfit.kwh)
```

To assess whether a linear model is appropriate for the data, the residual plots and Q-Q plots are observed and shown below. For linearity to be assumed, residuals should be homoscedastic. Based on the figure, there appears to be a slight pattern in the residuals. However, the points are scattered almost evenly around 0 and linearity can be assumed. The Q-Q plot will assess if the normality assumption is met. This plot shows that there is only deviation towards the tails and the rest of the pattern fits closely to a straight line, so the normality assumption is held.

```{r echo=FALSE}
res.kwh <- resid(pca.fit.kwh)
means.kwn <- kwh.dat$KWH.MEAN.2010[-which.min(res.kwh)]
res.kwh <- res.kwh[-which.min(res.kwh)]
summary.kwh <- summary(pca.fit.kwh)
```

```{r echo=FALSE, fig.width=6, fig.height=4}
par(mfrow=c(1,2))
plot(means.kwn, res.kwh, main = "Residual Plot",
     ylab = "Residuals", xlab = "kWh Mean", cex = 1)
abline(0,0)
qqnorm(res.kwh, cex = 1) # shows a heavy tailed distribution
qqline(res.kwh) 
```

The first 4 coefficients of the variables and the $R^2$ values are shown below. Assessing the fit of this model shows an $R^2$ value of .6658. This is considerably high and implies that 66.58% of variation in average kWh expenditure is explained by this model. The mean squared error is .156, which is also quite low and this provides evidence for good model fit. All of the variables are significant except certain categories of electricity accounts. This shows that building occupancy (PC1), building size (PC2), building age (PC3), building type and subtype, community area, and electricity accounts to an extent are significant in their relation to average electricity usage.
$~$

```{r}
summary.kwh$coefficients[1:5,]
summary.kwh$r.squared
mean(pca.fit.kwh$residuals^2)
```


Analyzing the coefficients shows some novel insights into the relationships between these variables and electricity. The second principal component has a positive coefficient, indicating that for an increase in building size, there is an increase in average kWh usage. This is not particularly surprising because larger buildings are expected to use more energy than smaller ones. Principal components 1 and 3 have negative coefficients indicating that an increase in building occupancy and building age result in a decrease in average kWh expenditure. This is surprising because older building and buildings with more occupants are assumed to use more electricity. Upon further [research](https://portfoliomanager.zendesk.com/hc/en-us/articles/211697247-Does-building-age-affect-energy-use-), it is understood that efficiency of appliances and electronics is more indicative of energy efficiency than building age. Therefore, it is possible for older buildings and buildings with larger occupancy to be more energy efficient and use less energy than newer ones. For all community areas, coefficients are negative with the most negative being the Far Southeast Side and the least negative being the North Side. This indicates that buildings in the Far Southeast Side may use, on average, less electricity amongst all community areas. 

### Analysis of Therms

This regression is repeated with Therms as the predictor. The Residual plot and Q-Q plot show a similar result from the analysis of kWh. Thus, although a slight trend is observed, it can be said that the linearity and normality assumptions are upheld. 

```{r echo=FALSE}
nums <- unlist(lapply(therms.dat, is.numeric))
pca.linearfit.therms <- data.frame("PC1" = prcomp.therms$x[,1], 
                                   "PC2" = prcomp.therms$x[,2], 
                                   "PC3" = prcomp.therms$x[,3], 
                                   therms.dat[, (nums == F)], 
                                   "THERM.MEAN.2010" = therms.dat$THERM.MEAN.2010)

pca.fit.therms <- lm(THERM.MEAN.2010 ~., dat = pca.linearfit.therms)
```

```{r echo=FALSE}
# Plots to check linearity
res.therms <- resid(pca.fit.therms)
means.therms <- therms.dat$THERM.MEAN.2010[-which.min(res.therms)]
res.therms <- res.therms[-which.min(res.therms)]
summary.therms <- summary(pca.fit.therms)
```

```{r echo=FALSE, fig.width=7.5, fig.height=3.5, eval=FALSE}
# save space
par(mfrow=c(1,2))
plot(means.therms, res.therms, main = "Residual Plot",
     ylab = "Residuals", xlab = "Therms Mean", cex = 1)
abline(0,0)
qqnorm(res.therms, cex = 1) # shows a heavy tailed distribution
qqline(res.therms)
```

The first 4 coefficients of the variables and the $R^2$ values are shown below. The $R^2$ value is .623. This is also a considerably large value and indicates that 62.33% of variation in average Therm expenditure is explained by this model. The mean squared error for this model is .123. This is even lower than the previous model indicating that there is a good model fit. All the variables are significant except for certain community area and gas accounts categories. The North Side and Southeast Side communities are not significant in modeling average Therm usage, but they are significant in modeling average kWh usage. Thus, all the same factors that are significant in analyzing kWh are also significant in analyzing Therms, but community areas differ in how they affect gas versus electricity usage. 
$~$

```{r}
summary.therms$coefficients[1:5,]
summary.therms$r.squared
mean(pca.fit.therms$residuals^2)
```

Principal components 1 and 2 have negative coefficients and component 3 has a positive coefficient. Thus building size (PC2) and building age (PC3) have the opposite effect on gas usage than they do on electricity usage. Additionally, the coefficients for community area are all positive whereas they are negative for the kWh model. 

These two linear models identified key features which are significant with average energy expenditures. These same features are significant with both kWh and Therms, and model fit measures provide strong evidence that the predictors explain the energy expenditure well. Certain variations do exist between how the variables relate to the different energy expenditures. Community areas significant in the kWh model are not significant in the Therm model and the sign of certain coefficients differ across the two models, indicating a reverse relationship. There is a better understanding of which variables contribute to understanding different energy expenditures and how they affect these expenditures. It can also be understood that the way in which these variables are related to energy expenditure depends on the type of energy being assessed. 

## Cubic Spline

To take into account the effects of months on energy expenditure, a non parametric model is analyzed. For this analysis, the constructed data described in the data exploration that shows the average values for each month for each observation is used. The months were changed to a numeric value between 1-12 and a cubic spline is done on months to predict the monthly energy usage for each building. The cubic spline creates non linear functions of months as new features. A linear regression is fit on these new features as well as the other numeric and categorical variables in the data set. This should result in more flexibility of the model. 

There are 3 knots initially chosen, which results in a basis for four regions meant to correspond to each season. Increasing the number of knots will increase the variance but decrease the bias of the model. Interaction terms are included to understand how the spline on months interacts with the other covariates in the data. 

### Analysis of kWh

The $R^2$ value that results from this initial model is .58 with a mean squared error of .47 when predicting monthly average energy usage.
$~$

```{r echo=FALSE}
levels(kwh.ms$months) <- c('1', '2', '3', '4','5', '6', '7', '8', '9', '10', '11', '12')
kwh.ms$months <- as.numeric(kwh.ms$months)
```

```{r}
bs.kwh <- lm(KWH.USE ~ .*bs(months, df=3, knots = 3), data=kwh.ms) 
```

```{r echo=FALSE}
summary.splines.kwh <- summary(bs.kwh)
```

```{r}
summary.splines.kwh$coefficients[1:4,]
summary.splines.kwh$r.squared
mean(bs.kwh$residuals^2)
```

However, the ultimate goal is to predict the yearly average energy while including the effects of months. Therefore, the average of 12 predicted values per observation is calculated to get the fitted values of the average energy use per year for each observation from the model. 
$~$

```{r}
fitted.kwh <- predict(bs.kwh)

kwh.mean.hat <- data.frame("KWH.MEAN.2010" = kwh.dat$KWH.MEAN.2010, "FITTED.MEAN" = 0)
for (obs in 1:nrow(therms.dat)){
  kwh.mean.hat[obs,2] <- sum(sapply(seq(0,11), 
                              function(x) fitted.kwh[(obs + x*nrow(kwh.dat))]))/12 
}
```

The mean squared error for the average yearly usage is 1.18.
$~$

```{r}
mean((kwh.mean.hat$KWH.MEAN.2010 - kwh.mean.hat$FITTED.MEAN)^2)
```

To improve this model, any collinearity in the variables in the model are penalized by Elastic Net regularization. An Elastic Net balances Ridge and Lasso regularization methods. Ridge uses an $l_2$ penalty which shrinks the coefficients in a model and is beneficial for addressing collinearity. Lasso regression also shrinks the coefficients in a model but uses an $l_1$ penalization. Lasso will also shrink some coefficients to 0, effectively doing variable selection in addition to shrinkage. A limitation of Lasso is that it tends to only pick one predictor when predictors are highly correlated. Additionally, the predictive performance of Lasso is less than Ridge when the number of observations are greater than the number of predictors. Using Elastic Net overcomes these challenges. An elastic net will use both an $l_1$ and $l_2$ penalization to penalize the coefficients. An elastic net thus serves as both a variable selection method and shrink the coefficients. The amount of shrinkage is chosen through tuning the lambda parameter. As the value of lambda increases, the coefficients are shrunk more, thus decreasing the variance but increasing the bias of the model. 
$~$

```{r}
set.seed(3)
elasticnet_kwh <- cv.glmnet(data.matrix(kwh.ms[,-which(colnames(kwh.ms) == "KWH.USE")]), 
                               kwh.ms$KWH.USE, nfolds = 10, alpha = .5)
```

```{r echo=FALSE}
coef <- as.matrix(coef(elasticnet_kwh, s = "lambda.1se"))
```

The left plot below shows the cross validation errors for increasing values of lambda. The right plot below shows the changes in the coefficients for increasing lambda values. 

```{r echo=FALSE, fig.width=6, fig.height=4}
par(mfrow = c(1, 2))
plot(elasticnet_kwh)
plot(elasticnet_kwh$glmnet.fit, "lambda")
```

Tuning this lambda parameter results in a removal of the Total Units variable. 

```{r, results='asis', echo=FALSE}
t1 <- kable(coef, format = "latex")
cat(c("\\begin{table}[h] \\centering ", 
      t1,
    "\\end{table}")) 
```

The best lambda value minimizes the cross validation error, and this value is .001. 
$~$

```{r}
min(elasticnet_kwh$lambda)
```

The model is again fit by removing this variable and by using 5 knots, which results in a basis being created for six regions. This will allow for the consideration of the effect of 2 months at a time. After increasing the number of knots and taking into consideration the elastic net penalization, the final model results in an $R^2$ value of .59 with a mean squared error of .47 in predicting the average energy use per month, and a mean squared error of 1.18 in predicting the average energy use per year. 
$~$

```{r}
bs.kwh <- lm(KWH.USE~(.- TOTAL.UNITS)*bs(months, df=3, knots = 5), data=kwh.ms) 
```

```{r echo=FALSE}
summary.splines.kwh <- summary(bs.kwh)
```

```{r}
summary.splines.kwh$coefficients[1:3,]
summary.splines.kwh$r.squared
mean(bs.kwh$residuals^2)
```

```{r echo=FALSE}
fitted.kwh <- predict(bs.kwh)

kwh.mean.hat <- data.frame("KWH.MEAN.2010" = kwh.dat$KWH.MEAN.2010, "FITTED.MEAN" = 0)
for (obs in 1:nrow(therms.dat)){
  kwh.mean.hat[obs,2] <- sum(sapply(seq(0,11), function(x) fitted.kwh[(obs + x*nrow(kwh.dat))]))/12 
}
```

```{r}
mean((kwh.mean.hat$KWH.MEAN.2010 - kwh.mean.hat$FITTED.MEAN)^2)
```


### Analysis of Therms

The procedure is repeated for Therms. The initial model results in an $R^2$ value of .81 with a mean squared error of .30 in predicting the average energy use per month, and a mean squared error of 1.73 in predicting the average energy use per year.

```{r echo=FALSE}
levels(therms.ms$months) <- c('1', '2', '3', '4','5', '6', '7', '8', '9', '10', '11', '12')
therms.ms$months <- as.numeric(therms.ms$months)
```
$~$

```{r}
bs.therms <- lm(THERM.USE ~ .*bs(months, df=3, knots = 3), data=therms.ms)
```

```{r echo=FALSE}
summary.splines.therms <- summary(bs.therms)
```

```{r}
summary.splines.therms$r.squared
mean(bs.therms$residuals^2)
```

```{r echo=FALSE}
fitted.therms <- predict(bs.therms)

therms.mean.hat <- data.frame("THERM.MEAN.2010" = therms.dat$THERM.MEAN.2010, "FITTED.MEAN" = 0)
for (obs in 1:nrow(therms.dat)){
  therms.mean.hat[obs,2] <- sum(sapply(seq(0,11), function(x) fitted.therms[(obs + x*nrow(therms.dat))]))/12 
}
```

```{r}
mean((therms.mean.hat$THERM.MEAN.2010 - therms.mean.hat$FITTED.MEAN)^2)
```

The elastic net resulted in the removal of more variables including Occupied Units, Occupied Housing Units, Building Subtype, Total Units, Average House Size, and Renter Occupied Housing Units as seen in the table below. Removing these variables greatly reduces the model complexity and variance in the model but also increases the bias. 

```{r echo=FALSE}
set.seed(3)
elasticnet_therms <- cv.glmnet(data.matrix(therms.ms[,-which(colnames(therms.ms) == "THERM.USE")]), 
                          therms.ms$THERM.USE, nfolds = 10, alpha = .5)
coef <- as.matrix(coef(elasticnet_therms, s = "lambda.1se"))
```

```{r, results='asis', echo=FALSE}
t1 <- kable(coef, format = "latex")
cat(c("\\begin{table}[h] \\centering ", 
      t1,
    "\\end{table}")) 
```

The lambda from the elastic net is also tuned to .001. 

```{r}
min(elasticnet_therms$lambda)
```

This final model results in an $R^2$ value of .80 with a mean squared error of .32 in predicting the average energy use per month, and a mean squared error of 1.70 in predicting the average energy use per year.
$~$

```{r}
bs.therms <- lm(THERM.USE~(.-OCCUPIED.UNITS - OCCUPIED.HOUSING.UNITS 
                - BUILDING_SUBTYPE - TOTAL.UNITS - AVERAGE.HOUSESIZE 
                - RENTER.OCCUPIED.HOUSING.UNITS)*bs(months, df=3, knots = 5), 
                data=therms.ms)
```

```{r echo=FALSE}
summary.splines.therms <- summary(bs.therms)
```

```{r}
summary.splines.therms$r.squared
mean(bs.therms$residuals^2)
```

```{r echo=FALSE}
fitted.therms <- predict(bs.therms)

therms.mean.hat <- data.frame("THERM.MEAN.2010" = therms.dat$THERM.MEAN.2010, "FITTED.MEAN" = 0)
for (obs in 1:nrow(therms.dat)){
  therms.mean.hat[obs,2] <- sum(sapply(seq(0,11), function(x) fitted.therms[(obs + x*nrow(therms.dat))]))/12 
}
```

```{r}
mean((therms.mean.hat$THERM.MEAN.2010 - therms.mean.hat$FITTED.MEAN)^2)
```

In both cases, the model does not improve much from the cubic spline model prior to regularization and is also comparatively worse than the linear regression using the principal components. Predicting energy usage while including the effects of months was expected to create a more accurate result of understanding energy usage at the higher level of a year. Therefore, it is surprising that the value of the mean squared error increased. One of the reasons for this increase may be due to a loss of information when calculating the average usage based on the months. Although this mean squared error is comparatively worse than the original linear model, almost all the variables are significant. This indicates that in addition to the numeric and categorical data, the effect of months is also related to average energy expenditure. Therefore, seasonality can  be said to have an effect on average energy expenditure which supports the exploratory data analysis and prior literature. 

## Random Forests 

A secondary goal of this analysis is to understand what factors are crucial when predicting building subtype. This is done through a random forest classification.

A major limitation of decision trees is that they are prone to overfitting and they are not robust, so a small change in the training data results in a different tree. Random forests generate many decision trees and aggregate their predictions into one single prediction to overcome these limitations. Additionally, random forest models tend to have better predictive performance than a single decision tree. Prior research attempting to understand and classify energy expenditure also determined that random forests perform better than other machine learning methods such as neural networks (Tso and Yao, 2007). For these reasons, a random forest model is used for classification of building subtype. 

Bootstrap aggregating, also known as bagging, will decorrelate the trees. Bagging will generate new training sets by sampling the original training set with replacement. Each bootstrapped data set will construct each tree and this decreases the variance without increasing the bias. At each splitting rule in a random forest model, a random subset of variables are considered. Additionally, the distance used in random forest is adaptive to the true underlying structure of the model. 

 Tuning is crucial when doing random forests and varying tuning parameters affect the bias and variance of the model. The terminal node size, mtry (the number of variables randomly chosen at each split from the full set of features), and the number of trees are tuned. The terminal node sizes being evaluated for tuning are (1,20,40,60), mtry are (1,3,6), and number of trees are (200, 500,1000). Tuning is done through a bootstrapped cross validation where the In-Bag samples are the training samples and the Out-of-Bag samples are testing samples. Combinations of the values are assessed and tuned by choosing the values that minimize the Out-of-Bag error estimate. 
 

### Analysis of kWh

Random forest is done on all the numeric variables mentioned earlier including average kWh as the features. Categorical variables are excluded since including categorical variables of different levels results in the random forests being biased towards the attributes with more levels. Prior to tuning, an Out-of-Bag error of 14.18% was achieved.

```{r echo=FALSE, eval=FALSE}
# Random Forest kWh -------------------------------------------------------
# Note that eval is set to false because it takes a lot of time to run
# and is not needed for specific results - this model is tuned
# OOB error is about 14%
rf.kwh <- randomForest(num.kwh, kwh.dat$BUILDING_SUBTYPE, 
                       ntree = 1000, mtry = 1, nodesize = 20, sampsize = 500)
```

Tuning of the parameters is done using the code displayed below:
$~$

```{r, eval=FALSE}
nodes <- c(1, 20, 40, 60) # picking some node values to test
mtrys <- c(1, 3, 6) # keeping the center value as the default 
ntrees <- c(200, 500, 1000)
best <- c(10,10,10, 10) # initating the best(error, mtry, nodesize)

for (ntree in ntrees){
  for (nodesize in nodes){
    for (mtry in mtrys){
      rf.fit <- randomForest(dat.numeric, kwh.dat$BUILDING_SUBTYPE, 
                             ntree = ntree, mtry = mtry, nodesize = nodesize)
      OOB.err <- mean(predict(rf.fit) != kwh.dat$BUILDING_SUBTYPE)
      # retain the information of only those predictors 
      # that have the lowest prediction error
      ifelse(OOB.err < best[1], best <- c(OOB.err, mtry, nodesize, ntree), best <- best)
      print(best)
    }
  }
}
```

After tuning the parameters, this error decreases to 9.7%. This is achieved at a node size of 1, tree size of 1000, and mtry of 6. The predictions resulting from the model are shown below.
$~$

```{r}
rf.kwh.tune <- randomForest(num.kwh, kwh.dat$BUILDING_SUBTYPE, 
                            ntree = 1000, mtry = 6, nodesize = 1, importance = TRUE)
```

```{r echo=FALSE}
rf.kwh.tune$confusion
```

The dominating group for building subtypes is single family subtypes that accounted for 51.1% of observations as described in the data exploration. The random forest model has a better prediction accuracy than 51.1%, indicating that using these features to classify building subtype did much better than if everything is classified into the dominating group. Additionally, the Mean Decrease in Accuracy and Mean Decrease in Gini are observed to determine the most important variables in the classifications. These values are shown in the plot below. 

```{r echo=FALSE, fig.width=6, fig.height=3.5}
par(mfrow = c(1, 2))
dotchart(sort(rf.kwh.tune$importance[,5]), main = "Mean Decrease Accuracy", cex = .5)
dotchart(sort(rf.kwh.tune$importance[,6]), main = "Mean Decrease Gini", cex = .5)
```

The mean decrease in accuracy assesses how much the accuracy of the random forest decreases due to the exclusion of each variable. The variables with a larger mean decrease in accuracy are therefore more important for classifying the Building Subtype. 

The Gini impurity measure is essentially the probability of a new record being incorrectly classified at a given node based on the training data. Mean Decrease in Gini is effectively a measure of how important a variable is for estimating the value of the target variable across all of the trees that make up the forest. A higher Mean Decrease in Gini therefore indicates higher variable importance. For both Mean Decrease in Accuracy and Mean Decrease in Gini, the variables with the largest value is average kWh per square foot followed by average stories. Since these variables are most crucial in the classification, they can be interpreted as having the most building subtype class discriminatory information. Average kWh per square foot expenditure having the largest variable importance means that this variable is distinctly different across building types. 

### Analysis of Therms

This random forest is done again but with Therms used in the features instead of kWh. Prior to tuning, an Out-of-Bag error of 15.21% was achieved. 

```{r echo=FALSE, eval=FALSE}
# Random Forest Therms -------------------------------------------------------

rf.therms <- randomForest(num.therms, therms.dat$BUILDING_SUBTYPE, 
                          ntree = 1000, mtry = 1, nodesize = 20, sampsize = 500)
```


After tuning the parameters, this error decreases to 9.7%. This is achieved at a node size of 20, tree size of 500, and mtry of 6. The prediction accuracy in this case is still better than 51.1%, indicating that using Therms in the features still classifies the building subtype better than if everything is classified into the dominating group of single family buildings. 

```{r echo=FALSE, eval=FALSE}
nodes <- c(1, 20, 40, 60) #picking some node values to test
mtrys <- c(1, 3, 6) #keeping the center value as the default 
ntrees <- c(200, 500, 1000)
best <- c(10,10,10, 10) #initating the best(error, mtry, nodesize)

for (ntree in ntrees){
  print(ntree)
  for (nodesize in nodes){
    print(nodesize)
    for (mtry in mtrys){
      print(mtry)
      rf.fit <- randomForest(num.therms, therms.dat$BUILDING_SUBTYPE, 
                             ntree = ntree, mtry = mtry, nodesize = nodesize)
      OOB.err <- mean(predict(rf.fit) != therms.dat$BUILDING_SUBTYPE)
      #retain the information of only those predictors that have the lowest prediction error
      ifelse(OOB.err < best[1], best <- c(OOB.err, mtry, nodesize, ntree), best <- best)
      print(best)
    }
  }
}

# 6.0000000  20.0000000 500.0000000

```
$~$

```{r}
rf.therms.tune <- randomForest(num.therms, therms.dat$BUILDING_SUBTYPE, 
                               ntree = 500, mtry = 6, nodesize = 20, importance = TRUE)
```

```{r echo=FALSE}
rf.therms.tune$confusion
```

For both Mean Decrease in Accuracy and Mean Decrease in Gini, the variables with the largest value are average Therms per square foot followed by average stories as seen below.
$~$

```{r echo=FALSE, fig.width=6, fig.height=3.5}
par(mfrow = c(1, 2))
dotchart(sort(rf.therms.tune$importance[,5]), main = "Mean Decrease Accuracy", cex = .5)
dotchart(sort(rf.therms.tune$importance[,6]), main = "Mean Decrease Gini", cex = .5)
```

 The results from this model show similar results as the previous one, indicating that in addition to average stories, average Therm usage per square foot provides most of the class discriminatory information for building subtype. 

Both random forest models provide information that average energy usage per square foot provides the most information for classification of building subtype. The high classification accuracy of the models provide evidence that these variables not only provide the most information for classification, but also result in classifications with low errors. This effectively suggests that building types are distinct across energy usage. 

\newpage

# Conclusion and Discussion

Two major goals were achieved through the various analyses. The first goal was to understand factors associated with two types of energy expenditure (electricity and gas). This was done through both parametric and non parametric models. The parametric linear regression identified features such as building occupancy size, building size, building age, building type and subtype, number of energy accounts, and community area to be significantly associated with energy usage. However, the level of significance and direction of the association varied between the two energy types. The non parametric regression aimed to assess the significance of months and consequently weather with energy expenditure. While this model performed worse than the parametric model, it shows that months have a significant association with energy usage, and thus usage varies during different times of the year. 

The second goal of the analysis was to classify Building Subtypes and understand the factors that contribute most to this classification. This classification through a random forest model resulted in a low prediction rate and the most significant variable in classifying Building Subtype was found to be average energy expenditure per square foot. This indicates that energy usage per square foot provides discriminatory information on different building subtypes and provides further evidence that energy usage can be explained by building subtype and vice versa. 

While these results provide insight into the factors that affect energy usage, there are limitations of the data and the analyses themselves. 

From the linear regression predicting average kWh usage, an increase in building age showed a negative association with energy usage. This result is justified by understanding that the efficiency of appliances and electronics used in buildings is more indicative of energy efficiency and usage than the age of the building. Since this information is not available in this dataset, using data that provides information of electronic and appliances used in each building could provide more accurate information on energy efficiency and usage. Additional data limitations of the data include having very few Industrial and Municipal building types and subtypes. This limitation prevented proper prediction of Municipal subtypes and Industrial building types in the analysis, which could have provided a potentially more accurate prediction. Furthermore, the data gives information only from 2010. More recent developments in housing and technology may also affect energy expenditure. Therefore, having more recent data would be beneficial in drawing an even stronger conclusion. 

Although the linear model generally met the linearity and normal assumptions, there is still a slight trend observed in the residuals. A major limitation of linear models is that they can model linear trends. The spline model provided a non parametric approach to modeling the data, but the accuracy of the model did not improve compared to the linear model. Investigating further non parametric methods such as support vector machines could provide better accuracy and performance as well as represent the trend in the data better than the linear model. To improve the classification accuracy of the random forest model, more parameter values could be tuned to reach an even lower Out-of-Bag error. Additionally, doing partial permutations and unbiased trees so that categorical variables can be included could potentially lead to better accuracy. 

By addressing these limitations in future research, further insight can be drawn about understanding factors affecting energy usage. By supplementing findings from this analysis with results from more accurate models or with data including more information, more concrete and specific information on energy usage can be understood. 

\newpage

# Sources

- Ozoh, P., et al. (2014). A Comparative Analysis of Techniques for Forecasting Electricity Consumption. International Journal of Computer Applications 88(15), pp. 8-12. 

- Ozoh, P., et al. (2016). A Predictive Framework for Electricity Consumption. Journal of IT in Asia 6(1). 

- Amasyali, K. & El-Gohary, M.N. (2018). A review of data-driven building energy consumption prediction studies. Renewable and Sustainable Energy Reviews 81(1), pp.1192-1205. 

- Tso, K.G. & Yao, K.K. (2007). Predicting electricity energy consumption: A comparison of regression analysis, decision tree and neural networks. Energy 32(9), pp.1761-1768. 

- Rahman, A., et al. (2018). Predicting electricity consumption for commercial and residential buildings using deep recurrent neural networks. Applied Energy 212, pp.372-385. 



